# TRANSFORMER-attention-is-all-you-need
From-scratch PyTorch reimplementation of the "Attention Is All You Need" Transformer (23M params) with pretraining + fine-tuning experiments.
