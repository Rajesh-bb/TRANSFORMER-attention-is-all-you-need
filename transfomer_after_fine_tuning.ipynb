{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of your models folder to confirm the file is there\n",
    "!ls /content/drive/MyDrive/MyModel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = '/content/drive/MyDrive/MyModel/ckpt_epoch40.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "n = 4\n",
    "head_size = d_model//n_heads\n",
    "max_len=512\n",
    "dropout_rate = 0.2\n",
    "# We pass max_len to the constructor\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Use max_len here\n",
    "        self.positional_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get B and T from the input tensor x INSIDE the method\n",
    "        B, T = x.shape\n",
    "        token_embedded = self.token_embedding(x)\n",
    "        # Use T to create the positions tensor\n",
    "        positional_embedded = self.positional_embedding(torch.arange(T, device=x.device))\n",
    "        return token_embedded + positional_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model,head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.d_model = d_model\n",
    "        self.query = nn.Linear(d_model,head_size,bias = False)\n",
    "        self.key = nn.Linear(d_model,head_size,bias = False)\n",
    "        self.value = nn.Linear(d_model,head_size,bias = False)\n",
    "    def forward(self,x,mask = None):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attention_score = torch.matmul(q,k.transpose(-2,-1))/(self.head_size**0.5)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attention_score = attention_score.masked_fill(mask==0,float('-inf'))\n",
    "        output = torch.matmul(torch.softmax(attention_score,dim=-1),v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,n_heads,d_model,head_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.multiheads = nn.ModuleList([SingleHeadSelfAttention(d_model,head_size) for head in range(n_heads)])\n",
    "        self.projection_layer = nn.Linear(d_model,d_model)\n",
    "    def forward(self,x,mask=None):\n",
    "        output = torch.cat([h(x,mask = mask) for h in self.multiheads],dim = -1)\n",
    "        output = self.projection_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model, head_size, max_len):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(d_model,head_size,bias = False)\n",
    "        self.key = nn.Linear(d_model,head_size,bias = False)\n",
    "        self.value = nn.Linear(d_model,head_size,bias = False)\n",
    "        # We create a large mask once and register it as a buffer.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(max_len, max_len)))\n",
    "    def forward(self,x):\n",
    "        B,T,d_model = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attention_score = torch.matmul(q,k.transpose(-2,-1))/(self.head_size**0.5)\n",
    "        # We slice the pre-computed buffer to match the current sequence length T.\n",
    "        masked_attention_score = attention_score.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        masked_attention_score = torch.softmax(masked_attention_score,dim=-1)\n",
    "        return masked_attention_score@v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self,n_heads,d_model,head_size,max_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.multiheads = nn.ModuleList([MaskedSingleHeadAttention(d_model,head_size,max_len) for head in range(n_heads)])\n",
    "        self.projection_layer = nn.Linear(d_model,d_model)\n",
    "    def forward(self,x):\n",
    "        output = torch.cat([h(x) for h in self.multiheads],dim = -1)\n",
    "        output = self.projection_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self,d_model,head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query = nn.Linear(d_model,head_size,bias = False)\n",
    "        self.key = nn.Linear(d_model,head_size,bias = False)\n",
    "        self.value = nn.Linear(d_model,head_size,bias = False)\n",
    "    def forward(self,encoder_output,masked_attention,mask = None):\n",
    "        q = self.query(masked_attention)\n",
    "        k = self.key(encoder_output)\n",
    "        v = self.value(encoder_output)\n",
    "        cross_attention_score = q@k.transpose(-2,-1)/(self.head_size**0.5)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            cross_attention_score = cross_attention_score.masked_fill(mask ==0,float('-inf'))\n",
    "        output = F.softmax(cross_attention_score,dim=-1)@v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self,n_heads,d_model,head_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.multiheads = nn.ModuleList([CrossAttention(d_model,head_size) for head in range(n_heads)])\n",
    "        self.projection_layer = nn.Linear(d_model,d_model)\n",
    "    def forward(self,encoder_output,masked_attention,mask=None):\n",
    "        output = torch.cat([h(encoder_output,masked_attention,mask=mask) for h in self.multiheads],dim = -1)\n",
    "        output = self.projection_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,d_model):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model,4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model,d_model)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        output = self.feed_forward(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,n_heads,d_model,head_size,dropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.multi_head_self_att = MultiHeadAttention(self.n_heads,self.d_model,self.head_size)\n",
    "        self.ffd = PositionWiseFeedForward(self.d_model)\n",
    "        self.ln1 = nn.LayerNorm(self.d_model)\n",
    "        self.ln2 = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "    def forward(self,x,mask = None):\n",
    "        x = self.ln1(x + self.dropout(self.multi_head_self_att(x,mask = mask)))\n",
    "        output = self.ln2(x + self.dropout(self.ffd(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n,n_heads,d_model,head_size,dropout_rate):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.blocks = nn.ModuleList([EncoderBlock(self.n_heads,self.d_model,self.head_size,self.dropout_rate) for _ in range(self.n)])\n",
    "    def forward(self,x,mask = None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x,mask = mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,n_heads,d_model,head_size,max_len,dropout_rate):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.masked_att = MaskedMultiHeadAttention(n_heads,d_model,head_size,max_len)\n",
    "        self.ffd = PositionWiseFeedForward(d_model)\n",
    "        self.cross_att = MultiHeadCrossAttention(n_heads,d_model,head_size)\n",
    "        # self.encoder = Encoder(n,n_heads,d_model,head_size,max_len)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    def forward(self,encoder_output,decoder_input,mask = None):\n",
    "        masked_attention = self.ln1(decoder_input + self.dropout(self.masked_att(decoder_input)))\n",
    "        cross_attention = self.ln2(masked_attention + self.dropout(self.cross_att(encoder_output,masked_attention,mask = mask)))\n",
    "        ffd_output = self.ln3(cross_attention + self.dropout(self.ffd(cross_attention)))\n",
    "        return ffd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n,n_heads,d_model,head_size,max_len,dropout_rate):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.blocks = nn.ModuleList([DecoderBlock(n_heads,d_model,head_size,max_len,dropout_rate) for i in range(n)])\n",
    "    def forward(self,encoder_output,decoder_input,mask = None):\n",
    "        for block in self.blocks:\n",
    "            decoder_input = block(encoder_output,decoder_input,mask = mask)\n",
    "        output = decoder_input\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_idx = tokenizer.bos_token_id  # BOS stands for \"Beginning Of Sequence\"\n",
    "eos_idx = tokenizer.eos_token_id\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,n,n_heads,d_model,head_size,max_len,dropout_rate,vocab_size):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.max_len = max_len\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.embedding = Embedding(vocab_size, d_model, max_len)\n",
    "        #self.embedding2 = Embedding(vocab_size, d_model, max_len)\n",
    "        self.encoder = Encoder(n,n_heads,d_model,head_size,dropout_rate)\n",
    "        self.decoder = Decoder(n,n_heads,d_model,head_size,max_len,dropout_rate)\n",
    "        self.final_layer = nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,dialogue,summary,mask = None):\n",
    "        embedded_dialogue = self.embedding(dialogue)\n",
    "        embedded_summary = self.embedding(summary)\n",
    "        encoder_output = self.encoder(embedded_dialogue,mask = mask)\n",
    "        decoder_output = self.decoder(encoder_output,embedded_summary, mask = mask)\n",
    "        logits = self.final_layer(decoder_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(n,n_heads,d_model,head_size,max_len,dropout_rate,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "# Load only the model's state dictionary from the checkpoint\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.to(device)\n",
    "print(\"âœ… Model weights successfully loaded from checkpoint!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"knkarthick/samsum\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "max_len = 512\n",
    "summary_len = 128\n",
    "batch_size = 16\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self,data,tokenizer,max_len,summary_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.summary_len = summary_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "        item = self.data[index]\n",
    "        dialogue = str(item['dialogue'])\n",
    "        summary = str(item['summary'])\n",
    "\n",
    "        encoder_inputs = tokenizer(dialogue,\n",
    "                                  max_length = self.max_len,\n",
    "                                  padding = 'max_length',\n",
    "                                  truncation = True,\n",
    "                                  return_tensors = 'pt'\n",
    "        )\n",
    "        decoder_inputs = tokenizer(tokenizer.bos_token+summary,\n",
    "                                  max_length = self.summary_len,\n",
    "                                  padding = 'max_length',\n",
    "                                  truncation = True,\n",
    "                                  return_tensors = 'pt')\n",
    "        labels = tokenizer(summary+tokenizer.eos_token,\n",
    "                           max_length = self.summary_len,\n",
    "                           padding = 'max_length',\n",
    "                           truncation = True,\n",
    "                           return_tensors = 'pt')\n",
    "        return {\n",
    "            \"encoder_input_ids\": encoder_inputs['input_ids'].flatten(),\n",
    "            \"encoder_attention_mask\": encoder_inputs['attention_mask'].flatten(),\n",
    "            \"decoder_input_ids\": decoder_inputs['input_ids'].flatten(),\n",
    "            \"labels\": labels['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "train_dataset = SummarizationDataset(dataset['train'],tokenizer,max_len = max_len,summary_len = summary_len)\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "valid_dataset = SummarizationDataset(dataset['validation'],tokenizer,max_len = max_len,summary_len = summary_len)\n",
    "validation_loader = DataLoader(valid_dataset,batch_size = batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "\n",
    "\n",
    "TOTAL_EPOCHS = 5                \n",
    "LEARNING_RATE = 3e-5            \n",
    "\n",
    "\n",
    "PRETRAIN_CKPT_PATH = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=0.1)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "start_epoch = 0\n",
    "if PRETRAIN_CKPT_PATH is not None:\n",
    "    if os.path.isfile(PRETRAIN_CKPT_PATH):\n",
    "        print(f\"Loading pretrained checkpoint from: {PRETRAIN_CKPT_PATH}\")\n",
    "        checkpoint = torch.load(PRETRAIN_CKPT_PATH, map_location=device)\n",
    "        #\n",
    "        if \"model_state_dict\" in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        if \"optimizer_state_dict\" in checkpoint:\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                print(\" Optimizer state loaded (if compatible).\")\n",
    "            except Exception:\n",
    "                print(\" Optimizer state could not be loaded (shape mismatch/optimizer differs).\")\n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "    else:\n",
    "        print(f\" Provided PRETRAIN_CKPT_PATH not found: {PRETRAIN_CKPT_PATH}. Starting from scratch.\")\n",
    "else:\n",
    "    print(\" No pretrained checkpoint path provided. Starting fine-tuning from current model weights.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"--- Starting fine-tuning for {TOTAL_EPOCHS} epochs ---\")\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        dialogue = batch['encoder_input_ids'].to(device)\n",
    "        summary = batch['decoder_input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        dialogue_mask = batch['encoder_attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(dialogue, summary, mask=dialogue_mask)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            dialogue = batch['encoder_input_ids'].to(device)\n",
    "            summary = batch['decoder_input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            dialogue_mask = batch['encoder_attention_mask'].to(device)\n",
    "\n",
    "            logits = model(dialogue, summary, mask=dialogue_mask)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_valid_loss = total_valid_loss / len(validation_loader)\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Epoch: {epoch + 1:02}/{TOTAL_EPOCHS} | Time: {epoch_duration:.2f}s\")\n",
    "    print(f\"\\tTrain Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"\\t Val. Loss: {avg_valid_loss:.4f}\")\n",
    "\n",
    "    \n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"--- Fine-tuning complete ---\")\n",
    "\n",
    "final_ckpt = {\n",
    "    \"epoch\": TOTAL_EPOCHS,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}\n",
    "FINAL_SAVE_PATH = \"/content/best_finetuned.pt\"   # Colab content\n",
    "torch.save(final_ckpt, FINAL_SAVE_PATH)\n",
    "print(f\"ðŸ”– Saved final checkpoint to: {FINAL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE FINAL MODEL WEIGHTS ---\n",
    "torch.save(model.state_dict(), \"/content/best_finetuned.pt\")\n",
    "print(\" Saved final model weights to /content/best_finetuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1) Re-create your model architecture exactly as before\n",
    "model = Transformer(\n",
    "    n=n,\n",
    "    n_heads=n_heads,\n",
    "    d_model=d_model,\n",
    "    head_size=head_size,\n",
    "    max_len=max_len,\n",
    "    dropout_rate=dropout_rate,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# 2) Load the saved state dict\n",
    "checkpoint_path = \"/content/best_finetuned.pt\"\n",
    "state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 3) Switch to eval mode & move to device\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dialogue = dataset['test']['dialogue'][6]\n",
    "test_summary = dataset['test']['summary'][6]\n",
    "print(type(test_dialogue))\n",
    "print(type(test_summary))\n",
    "print(\"#### the dialogue\")\n",
    "print(test_dialogue)\n",
    "print(\"#### the summary\")\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model: Transformer, dialogue: str, max_len: int = 200) -> str:\n",
    "    tok_out = tokenizer(dialogue, return_tensors=\"pt\").to(device)\n",
    "    encoder_ids = tok_out[\"input_ids\"]\n",
    "    output_ids = torch.tensor([tokenizer.bos_token_id], device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = model(encoder_ids, output_ids)\n",
    "        next_logits = logits[:, -1, :]\n",
    "        probs = nn.functional.softmax(next_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        output_ids = torch.cat([output_ids, next_id], dim=-1)\n",
    "        if next_id.item() == eos_idx:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(output_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "summary = generate(model, test_dialogue)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
